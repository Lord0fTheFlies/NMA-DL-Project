{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Network 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                    diseases  shortness of breath  sharp chest pain  \\\n",
       " 0  abdominal aortic aneurysm                    1                 0   \n",
       " 1  abdominal aortic aneurysm                    1                 0   \n",
       " 2  abdominal aortic aneurysm                    0                 0   \n",
       " 3  abdominal aortic aneurysm                    0                 0   \n",
       " 4  abdominal aortic aneurysm                    0                 0   \n",
       " \n",
       "    dizziness  cough  skin swelling  leg pain  sharp abdominal pain  vomiting  \\\n",
       " 0          0      0              0         0                     1         0   \n",
       " 1          0      0              0         0                     1         0   \n",
       " 2          0      0              0         0                     1         0   \n",
       " 3          0      0              0         0                     0         0   \n",
       " 4          0      0              0         0                     1         0   \n",
       " \n",
       "    headache  nausea  \n",
       " 0         0       0  \n",
       " 1         0       0  \n",
       " 2         0       0  \n",
       " 3         0       0  \n",
       " 4         0       0  ,\n",
       "                     diseases  anxiety and nervousness  depression  \\\n",
       " 0  abdominal aortic aneurysm                        0           0   \n",
       " 1  abdominal aortic aneurysm                        0           0   \n",
       " 2  abdominal aortic aneurysm                        0           0   \n",
       " 3  abdominal aortic aneurysm                        0           0   \n",
       " 4  abdominal aortic aneurysm                        0           0   \n",
       " \n",
       "    shortness of breath  depressive or psychotic symptoms  sharp chest pain  \\\n",
       " 0                    1                                 0                 0   \n",
       " 1                    1                                 0                 0   \n",
       " 2                    0                                 0                 0   \n",
       " 3                    0                                 0                 0   \n",
       " 4                    0                                 0                 0   \n",
       " \n",
       "    dizziness  insomnia  abnormal involuntary movements  chest tightness  ...  \\\n",
       " 0          0         0                               0                0  ...   \n",
       " 1          0         0                               0                0  ...   \n",
       " 2          0         0                               0                0  ...   \n",
       " 3          0         0                               0                0  ...   \n",
       " 4          0         0                               0                0  ...   \n",
       " \n",
       "    upper abdominal pain  difficulty breathing  chills  fatigue  \\\n",
       " 0                     0                     0       0        0   \n",
       " 1                     0                     0       0        0   \n",
       " 2                     0                     0       0        0   \n",
       " 3                     0                     0       0        0   \n",
       " 4                     0                     0       0        0   \n",
       " \n",
       "    delusions or hallucinations  temper problems  coryza  itching of skin  \\\n",
       " 0                            0                0       0                0   \n",
       " 1                            0                0       0                0   \n",
       " 2                            0                0       0                0   \n",
       " 3                            0                0       0                0   \n",
       " 4                            0                0       0                0   \n",
       " \n",
       "    skin dryness, peeling, scaliness, or roughness  skin rash  \n",
       " 0                                               0          0  \n",
       " 1                                               0          0  \n",
       " 2                                               0          0  \n",
       " 3                                               0          0  \n",
       " 4                                               0          0  \n",
       " \n",
       " [5 rows x 101 columns])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_10_features = pd.read_csv('/Users/eileenlehane/Documents/Computer_science/Projects/NMA-DL-Project/KNN_df_10_feature_reduced.csv')\n",
    "data_100_features = pd.read_csv('/Users/eileenlehane/Documents/Computer_science/Projects/NMA-DL-Project/KNN_df_100_feature_reduced.csv')\n",
    "\n",
    "data_10_features.head(), data_100_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in 10 features: 527\n",
      "Number of classes in 100 features: 527\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_10_features = data_10_features.drop('diseases', axis=1)\n",
    "y_10_features = data_10_features['diseases']\n",
    "\n",
    "X_100_features = data_100_features.drop('diseases', axis=1)\n",
    "y_100_features = data_100_features['diseases']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_10_features = label_encoder.fit_transform(y_10_features)\n",
    "y_100_features = label_encoder.fit_transform(y_100_features)\n",
    "\n",
    "num_classes_10 = len(label_encoder.classes_)\n",
    "num_classes_100 = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Number of classes in 10 features: {num_classes_10}\")\n",
    "print(f\"Number of classes in 100 features: {num_classes_100}\")\n",
    "\n",
    "print(y_10_features[:10])  \n",
    "print(y_100_features[:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_10, X_test_10, y_train_10, y_test_10 = train_test_split(X_10_features, y_10_features, test_size=0.2, random_state=42)\n",
    "X_train_100, X_test_100, y_train_100, y_test_100 = train_test_split(X_100_features, y_100_features, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# Feature Engineering: Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train_10_poly = poly.fit_transform(X_train_10)\n",
    "X_test_10_poly = poly.transform(X_test_10)\n",
    "X_train_100_poly = poly.fit_transform(X_train_100)\n",
    "X_test_100_poly = poly.transform(X_test_100)\n",
    "\n",
    "# Data Normalization\n",
    "scaler_10 = StandardScaler()\n",
    "X_train_10_scaled = scaler_10.fit_transform(X_train_10_poly)\n",
    "X_test_10_scaled = scaler_10.transform(X_test_10_poly)\n",
    "\n",
    "scaler_100 = StandardScaler()\n",
    "X_train_100_scaled = scaler_100.fit_transform(X_train_100_poly)\n",
    "X_test_100_scaled = scaler_100.transform(X_test_100_poly)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_10_tensor = torch.tensor(X_train_10_scaled, dtype=torch.float32)\n",
    "X_test_10_tensor = torch.tensor(X_test_10_scaled, dtype=torch.float32)\n",
    "X_train_100_tensor = torch.tensor(X_train_100_scaled, dtype=torch.float32)\n",
    "X_test_100_tensor = torch.tensor(X_test_100_scaled, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_10 = StandardScaler()\n",
    "X_train_10_scaled = scaler_10.fit_transform(X_train_10)\n",
    "X_test_10_scaled = scaler_10.transform(X_test_10)\n",
    "\n",
    "scaler_100 = StandardScaler()\n",
    "X_train_100_scaled = scaler_100.fit_transform(X_train_100)\n",
    "X_test_100_scaled = scaler_100.transform(X_test_100)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_10_tensor = torch.tensor(X_train_10_scaled, dtype=torch.float32)\n",
    "X_test_10_tensor = torch.tensor(X_test_10_scaled, dtype=torch.float32)\n",
    "X_train_100_tensor = torch.tensor(X_train_100_scaled, dtype=torch.float32)\n",
    "X_test_100_tensor = torch.tensor(X_test_100_scaled, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_10_tensor = torch.tensor(X_train_10.values, dtype=torch.float32)\n",
    "y_train_10_tensor = torch.tensor(y_train_10, dtype=torch.long)\n",
    "X_test_10_tensor = torch.tensor(X_test_10.values, dtype=torch.float32)\n",
    "y_test_10_tensor = torch.tensor(y_test_10, dtype=torch.long)\n",
    "\n",
    "X_train_100_tensor = torch.tensor(X_train_100.values, dtype=torch.float32)\n",
    "y_train_100_tensor = torch.tensor(y_train_100, dtype=torch.long)\n",
    "X_test_100_tensor = torch.tensor(X_test_100.values, dtype=torch.float32)\n",
    "y_test_100_tensor = torch.tensor(y_test_100, dtype=torch.long)\n",
    "\n",
    "train_dataset_10 = TensorDataset(X_train_10_tensor, y_train_10_tensor)\n",
    "test_dataset_10 = TensorDataset(X_test_10_tensor, y_test_10_tensor)\n",
    "train_loader_10 = DataLoader(train_dataset_10, batch_size=32, shuffle=True)\n",
    "test_loader_10 = DataLoader(test_dataset_10, batch_size=32, shuffle=False)\n",
    "\n",
    "train_dataset_100 = TensorDataset(X_train_100_tensor, y_train_100_tensor)\n",
    "test_dataset_100 = TensorDataset(X_test_100_tensor, y_test_100_tensor)\n",
    "train_loader_100 = DataLoader(train_dataset_100, batch_size=32, shuffle=True)\n",
    "test_loader_100 = DataLoader(test_dataset_100, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Fit a Random Forest model to the data\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_100, y_train_100)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Select the top 50 features\n",
    "indices = np.argsort(importances)[-50:]\n",
    "X_train_100_selected = X_train_100.iloc[:, indices]\n",
    "X_test_100_selected = X_test_100.iloc[:, indices]\n",
    "\n",
    "# Normalize the selected features\n",
    "scaler_100 = StandardScaler()\n",
    "X_train_100_scaled = scaler_100.fit_transform(X_train_100_selected)\n",
    "X_test_100_scaled = scaler_100.transform(X_test_100_selected)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_100_tensor = torch.tensor(X_train_100_scaled, dtype=torch.float32)\n",
    "X_test_100_tensor = torch.tensor(X_test_100_scaled, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, output_dim)\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        if self.output_dim == 1:\n",
    "            x = torch.sigmoid(self.fc4(x))\n",
    "        else:\n",
    "            x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "output_dim_10 = 1 if num_classes_10 == 2 else num_classes_10\n",
    "output_dim_100 = 1 if num_classes_100 == 2 else num_classes_100\n",
    "\n",
    "model_10 = NeuralNet(X_train_10_tensor.shape[1], output_dim_10)\n",
    "model_100 = NeuralNet(X_train_100_tensor.shape[1], output_dim_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ImprovedNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ImprovedNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, output_dim)\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        if self.output_dim == 1:\n",
    "            x = torch.sigmoid(self.fc4(x))\n",
    "        else:\n",
    "            x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the models with increased depth\n",
    "model_10 = ImprovedNeuralNet(X_train_10_tensor.shape[1], output_dim_10)\n",
    "model_100 = ImprovedNeuralNet(X_train_100_tensor.shape[1], output_dim_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EnhancedNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(EnhancedNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 2048)\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.bn2 = nn.BatchNorm1d(1024)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.fc4 = nn.Linear(512, output_dim)\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        if self.output_dim == 1:\n",
    "            x = torch.sigmoid(self.fc4(x))\n",
    "        else:\n",
    "            x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the models with increased depth and batch normalization\n",
    "model_10 = EnhancedNeuralNet(X_train_10_tensor.shape[1], output_dim_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if output_dim_10 > 1:\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs.view(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "if output_dim_10 > 1:\n",
    "    criterion_10 = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_10 = nn.BCELoss()\n",
    "\n",
    "if output_dim_100 > 1:\n",
    "    criterion_100 = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_100 = nn.BCELoss()\n",
    "\n",
    "optimizer_10 = optim.Adam(model_10.parameters(), lr=0.001)\n",
    "optimizer_100 = optim.Adam(model_100.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model_10, train_loader_10, criterion_10, optimizer_10)\n",
    "train_model(model_100, train_loader_100, criterion_100, optimizer_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if model.output_dim > 1:\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs.view(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Initialize loss function, optimizer, and scheduler\n",
    "if output_dim_10 > 1:\n",
    "    criterion_10 = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_10 = nn.BCELoss()\n",
    "\n",
    "if output_dim_100 > 1:\n",
    "    criterion_100 = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_100 = nn.BCELoss()\n",
    "\n",
    "optimizer_10 = optim.Adam(model_10.parameters(), lr=0.001)\n",
    "scheduler_10 = optim.lr_scheduler.StepLR(optimizer_10, step_size=5, gamma=0.1)\n",
    "\n",
    "optimizer_100 = optim.Adam(model_100.parameters(), lr=0.001)\n",
    "scheduler_100 = optim.lr_scheduler.StepLR(optimizer_100, step_size=5, gamma=0.1)\n",
    "\n",
    "# Train the models with more epochs and learning rate scheduler\n",
    "train_model(model_10, train_loader_10, criterion_10, optimizer_10, scheduler_10)\n",
    "train_model(model_100, train_loader_100, criterion_100, optimizer_100, scheduler_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if model.output_dim > 1:\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs.view(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Initialize loss function, optimizer, and scheduler\n",
    "if output_dim_10 > 1:\n",
    "    criterion_10 = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_10 = nn.BCELoss()\n",
    "\n",
    "if output_dim_100 > 1:\n",
    "    criterion_100 = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_100 = nn.BCELoss()\n",
    "\n",
    "optimizer_10 = optim.Adam(model_10.parameters(), lr=0.001)\n",
    "scheduler_10 = optim.lr_scheduler.StepLR(optimizer_10, step_size=10, gamma=0.1)\n",
    "\n",
    "optimizer_100 = optim.Adam(model_100.parameters(), lr=0.001)\n",
    "scheduler_100 = optim.lr_scheduler.StepLR(optimizer_100, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train the models with more epochs and learning rate scheduler\n",
    "train_model(model_10, train_loader_10, criterion_10, optimizer_10, scheduler_10)\n",
    "train_model(model_100, train_loader_100, criterion_100, optimizer_100, scheduler_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.8709\n",
      "Epoch 2/50, Loss: 0.8675\n",
      "Epoch 3/50, Loss: 0.8635\n",
      "Epoch 4/50, Loss: 0.8561\n",
      "Epoch 5/50, Loss: 0.8576\n",
      "Epoch 6/50, Loss: 0.8523\n",
      "Epoch 7/50, Loss: 0.8428\n",
      "Epoch 8/50, Loss: 0.8442\n",
      "Epoch 9/50, Loss: 0.8445\n",
      "Epoch 10/50, Loss: 0.8360\n",
      "Epoch 11/50, Loss: 0.8354\n",
      "Epoch 12/50, Loss: 0.8279\n",
      "Epoch 13/50, Loss: 0.8216\n",
      "Epoch 14/50, Loss: 0.8204\n",
      "Epoch 15/50, Loss: 0.8234\n",
      "Epoch 16/50, Loss: 0.8170\n",
      "Epoch 17/50, Loss: 0.8116\n",
      "Epoch 18/50, Loss: 0.8101\n",
      "Epoch 19/50, Loss: 0.8064\n",
      "Epoch 20/50, Loss: 0.8015\n",
      "Epoch 21/50, Loss: 0.7987\n",
      "Epoch 22/50, Loss: 0.8028\n",
      "Epoch 23/50, Loss: 0.7943\n",
      "Epoch 24/50, Loss: 0.7942\n",
      "Epoch 25/50, Loss: 0.7910\n",
      "Epoch 26/50, Loss: 0.7923\n",
      "Epoch 27/50, Loss: 0.7940\n",
      "Epoch 28/50, Loss: 0.7834\n",
      "Epoch 29/50, Loss: 0.7797\n",
      "Epoch 30/50, Loss: 0.7821\n",
      "Epoch 31/50, Loss: 0.7759\n",
      "Epoch 32/50, Loss: 0.7767\n",
      "Epoch 33/50, Loss: 0.7720\n",
      "Epoch 34/50, Loss: 0.7678\n",
      "Epoch 35/50, Loss: 0.7700\n",
      "Epoch 36/50, Loss: 0.7667\n",
      "Epoch 37/50, Loss: 0.7649\n",
      "Epoch 38/50, Loss: 0.7652\n",
      "Epoch 39/50, Loss: 0.7684\n",
      "Epoch 40/50, Loss: 0.7593\n",
      "Epoch 41/50, Loss: 0.7545\n",
      "Epoch 42/50, Loss: 0.7556\n",
      "Epoch 43/50, Loss: 0.7537\n",
      "Epoch 44/50, Loss: 0.7476\n",
      "Epoch 45/50, Loss: 0.7498\n",
      "Epoch 46/50, Loss: 0.7532\n",
      "Epoch 47/50, Loss: 0.7450\n",
      "Epoch 48/50, Loss: 0.7510\n",
      "Epoch 49/50, Loss: 0.7457\n",
      "Epoch 50/50, Loss: 0.7400\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if model.output_dim > 1:\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs.view(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step(running_loss / len(train_loader.dataset))\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Initialize loss function, optimizer, and scheduler\n",
    "if output_dim_100 > 1:\n",
    "    criterion_100 = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion_100 = nn.BCELoss()\n",
    "\n",
    "optimizer_100 = optim.Adam(model_100.parameters(), lr=0.001)\n",
    "scheduler_100 = ReduceLROnPlateau(optimizer_100, 'min', patience=5, verbose=True)\n",
    "\n",
    "# Train the models with more epochs and learning rate scheduler\n",
    "train_model(model_100, train_loader_100, criterion_100, optimizer_100, scheduler_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            if model.output_dim > 1:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            else:\n",
    "                predicted = (outputs.view(-1) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "accuracy_10 = evaluate_model(model_10, test_loader_10)\n",
    "accuracy_100 = evaluate_model(model_100, test_loader_100)\n",
    "\n",
    "print(f\"10-feature model accuracy: {accuracy_10:.2f}%\")\n",
    "print(f\"100-feature model accuracy: {accuracy_100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            if model.output_dim > 1:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            else:\n",
    "                predicted = (outputs.view(-1) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_10 = evaluate_model(model_10, test_loader_10)\n",
    "accuracy_100 = evaluate_model(model_100, test_loader_100)\n",
    "\n",
    "print(f\"10-feature model accuracy: {accuracy_10:.2f}%\")\n",
    "print(f\"100-feature model accuracy: {accuracy_100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-feature model accuracy: 6.41%\n",
      "100-feature model accuracy: 65.45%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            if model.output_dim > 1:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            else:\n",
    "                predicted = (outputs.view(-1) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_10 = evaluate_model(model_10, test_loader_10)\n",
    "accuracy_100 = evaluate_model(model_100, test_loader_100)\n",
    "\n",
    "print(f\"10-feature model accuracy: {accuracy_10:.2f}%\")\n",
    "print(f\"100-feature model accuracy: {accuracy_100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100-feature model accuracy: 65.45%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            if model.output_dim > 1:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            else:\n",
    "                predicted = (outputs.view(-1) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_100 = evaluate_model(model_100, test_loader_100)\n",
    "\n",
    "print(f\"100-feature model accuracy: {accuracy_100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_10.state_dict(), 'text_classification_model_10_features.pth')\n",
    "torch.save(model_100.state_dict(), 'text_classification_model_100_features.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
